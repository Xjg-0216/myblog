## 机器学习基础知识

[TOC]

### 1、线性回归

我们的目标是建立一个系统，将向量$x_i$作为输入， 预测标量$y\in R$作为输出，线性回归的输出是其输入的线性函数，令$f(x_i)$表示模型预测y应该取得的值，我们定义输出为：$f(x) = wx_i + b$

其中，$w_i$是参数向量, b为偏置

#### 均方误差

如何确定$w$和$b$呢，显然，关键在于如何衡量$f(x)$与y之间的差别，我们可以通过**均方误差**来试图让均方误差最小化，即：

$(w^*,b^*) = arg min_{(w, b)}\sum^{m}_{i=1}(f(x_i)-y_i)^2$

均方误差对应了常用的欧几里得距离，基于均方误差最小化来进行模型求解的方法称为最小二乘法， 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。

**求解$w$和$b$**：

分别对$w$和$b$求导，然后再对求导后的式子令其为0， 可得到$w$和$b$的最优的闭式解。

更一般的我们可以扩展到多元线性回归，类似的求解步骤，但是由于$X^TX$在现实任务中往往不是满秩矩阵，此时可能会解出多个$\hat{w}$, 它们都能使得均方误差最小化，选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入**正则化项**

另一种用来估计概率分布的参数值的方法是**极大似然估计**

#### 极大似然估计

​    具体定义：对于离散型（连续型）随机变量X， 假设其概率质量函数为$P(x;\theta)$(连续的为概率密度函数)，其中$\theta$为待估计的参数值，现有$x_1, x_2, x_3,...,x_n$是来自X的n个独立同分布的样本，它们的联合概率为$L(\theta) = \prod^n_{i=1}P(x_i;\theta)$

其中$x_1, x_2, x_3,...,x_n$是已知量，$\theta$是未知量，因此以上概率是一个关于$\theta$的函数，称为$L(\theta)$为样本的似然函数。极大似然估计的直观想法是：使得观测样本出现概率最大的分布就是待求分布，也即使得似然函数$L(\theta)$取得最大值的$\theta^*$即为$\theta$的估计值。

通过极大似然估计的方法我们同样可以得出与通过均方误差相同的结果，即：

$(w^*,b^*)=argmax_{(w,b)} ln L(w, b)=argmin_{w,b}\sum^m_{i=1}(y_i-wx_i-b)^2$

求解过程同最小二乘法相同。

### 2、逻辑回归（对数几率回归）

为了做分类任务我们可以通过寻找一个单调可微函数将分类任务的真实标记与线性回归模型的预测值联系起来

**为什么选用sigmoid函数做作为联系函数？**

【最大熵的角度】

https://zhuanlan.zhihu.com/p/59519202

https://transwarpio.github.io/teaching_ml/2017/08/15/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/

[Maximum Entropy Model最大熵模型 - 简书](https://www.jianshu.com/p/10d778068f70)

LR的本质为二分类下选取了某个特征函数的最大熵模型。

推导出损失函数：可以利用**极大似然**和**信息论**的角度进行推导

#### 极大似然估计

首先确定概率质量函数（离散的为例）,已知离散型随机变量$y\in\{0,1\}$取值为1和0的概率分别建模为

$p(y=1|\hat{x};\beta)=\frac{e^{\beta^T\hat{x}}}{1+e^{\beta^T\hat{x}}}=p_1(\hat{x};\beta)$

$p(y=0|\hat{x};\beta)=\frac{1}{1+e^{\beta^T\hat{x}}}=p_0(\hat{x};\beta)$

由以上概率取值可推得随机变量$y\in\{0,1\}$的概率质量函数为

$p(y|\hat{x};\beta)=y* p_1(\hat{x};\beta)+(1-y)*p_0(\hat{x};\beta)$

似然函数为：$L(\beta)=\prod^m_{i=1}p(y_i|\hat{x_i};\beta)$

进一步对数似然为：$l(\beta)=lnL(\beta)=\sum^m_{i=1}ln p(y_i|\hat{x_i};\beta)$

$l(\beta)=lnL(\beta)=\sum^m_{i=1}ln y*p_1(\hat{x_i};\beta)+(1-y_i)*p_0(\hat{x_i};\beta)$

带入p1和p0，我们可以得到最终的损失函数：

$l(\beta)=\sum^m_{i=1}(y_i\beta^T\hat{x_i}-ln(1+e^{\beta^T\hat{x_i}}))$

由于损失函数通常为以最小化为优化目标，因此可以将最大化$l(\beta)$等价于最小化$-l(\beta)$

#### 信息论

相对熵（KL散度）：度量两个分布的差异，其典型使用场景是用来度量理想分布$p(x)$和模拟分布$q(x)$之间的差异。

$D_{KL}(p||q)=\sum_xp(x)log_bp(x)-\sum_xp(x)log_bq(x)$

从机器学习三要素中“策略”的角度来说，与理想分布最接近的模拟分布即为最优分布，因此可以通过最小化相对熵这个策略来求最优分布，由于理想分布p(x)是未知但固定的分布，所以$\sum_xp(x)log_bp(x)$为常量，那么最小化相对熵等价于最小化交叉熵

那么单个样本$y_i$的交叉熵为：

$-\sum_xp(x)log_bq(x)$

$-\sum_{y_i}p(y_i)log_bq(y_i) = -p(1)*log_bp_1(\hat{x};\beta)-p(0)*log_bp_0(\hat{x};\beta)=-y_i*log_bp_1(\hat{x};\beta)-(1-y_i)*log_bp_0(\hat{x};\beta)$

换成以e为底

$-y_i*lnp_1(\hat{x};\beta)-(1-y_i)*lnp_0(\hat{x};\beta)$

全体训练样本的交叉熵为

$\sum^m_{i=1}[-y_ilnp_1(\hat{x_i};\beta)-(1-y_i)lnp_0(\hat{x_i};\beta)]$

类似地，带入p1和p0

$=\sum^m_{i=1}[-y_iln(e^{\beta^T\hat{x_i}}-ln(\frac{1}{1+e^{\beta^T\hat{x_i}}}))]=\sum^m_{i=1}(-y_i\beta^T\hat{x_i}+ln(1+e^{\beta^T\hat{x_i}}))$

同样也可以推导出相同的结果，该式为高阶可导连续凸函数，可以根据**梯度下降法**、**牛顿法**等进行求解最优解。

### 3、贝叶斯分类器

#### 贝叶斯决策论

贝叶斯决策论是概率框架下实施决策的基本方法，对分类任务来说，在所有相关概率都已知的理想清形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

假设有N种可能的类别标记，即$y=\{c_1, c_2,c_3,...,c_N\}$,$\lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失，基于后验概率$P(c_i|x)$可获得将样本x分类为$c_i$所产生的期望损失，即在样本x上的“条件风险”

$R(c_i|x)=\sum^N_{j=1}\lambda_{ij}P(c_j|x)$

全部样本构成的总体风险为：

$R(h)=E_x[R(h(x)|x)]$

其中，h为分类器模型，显然，分类效果越准确的h，其条件风险和总体风险也越小。

贝叶斯判定准则：为最小化总体风险R(h)，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记，即：

$h^*(x)=argmin_{c\in y}P(c|x)$

此时，$h^*$称为贝叶斯最优分类器

具体地，若目标是最小化分类错误率，误判损失$\lambda_{ij}$在i=j的时候为0，其他情况下为1

此时，单个样本x的期望损失（条件风险）为

$R(c_i|x)=\sum^N_{j=1}\lambda_{ij}P(c_j|x)$

又因为$\sum^N_{j=1}P(c_j|x)=1$,则$R(c_i|x)=1-P(c_i|x)$

于是，按照上述的推导，最小化分类错误率的贝叶斯最有分类器等价于 **最大化后验概率**

从贝叶斯决策论的角度：机器学习所要做的就是基于有限的训练样本集尽可能准确地估计出后验概率$P(c|x)$

从机器学习自己的角度：给定一个样本x， 求一个能准确分类x的f(x)，其有些算法可以看作是对后验概率建模P(c|x)，而有些算法则是纯粹完成样本分类（SVM）

************

判别式模型：给定x, 直接建模P(c|x)来预测c

生成式模型：先对联合概率P(x,c)建模，然后再由此推导出P(c|x)

******

对于生成式模型，其建模思路为：

$P(c|x)=\frac{P(x,c)}{P(x)}$

再根据贝叶斯定理，上式可恒等变形为：

$P(c|x)=\frac{P(c)P(x|c)}{P(x)}$

其中，P(c)是类先验概率，P(x|c)是样本x相对于类别标记c的类条件概率(似然函数)，P(x)是用于归一化的证据因子

#### 朴素贝叶斯分类器

基于贝叶斯公式来估计后验概率的主要困难在于：似然函数是所有属性的联合概率。

**属性条件独立性假设：对已知类别，假设所有属性（特征）相互独立**

$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod^d_{i=1}P(x_i|c)$

其中，d为属性数目,$x_i$为x在第i个属性上的取值，基于贝叶斯判定准则：

$h^*(x)=argmax_{c\in y}P(c|x)=argmax_{c\in y}\frac{P(c)}{P(x)}\prod^d_{i=1}P(x_i|c)$

由于对所有类别来说P(x)都相同，所以P(x)视为常量可以略去，则朴素贝叶斯最优分类器为：

$h_{nb}(x)=argmax_{c\in y}P(c)\prod^d_{i=1}P(x_i|c)$

从而等价于含$\theta$参数的极大似然估计

#### 半朴素贝叶斯分类器

为了降低贝叶斯公式中估计后验概率的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立，半朴素贝叶斯分类器的基本想法是：**适当考虑一部分属性间的相互依赖信息，从而既不需进行完全概率计算，又不至于彻底忽略了比较强的属性依赖关系**，独依赖估计是半朴素贝叶斯分类器最常见的策略：假设每个属性在类别之外最多仅依赖于一个其他属性，即：

$P(c|x) = P(c)\prod^d_{i=1}P(x_i|c,pa_i)$

### 4、线性判别分析（LDA）

从几何的角度，让全体训练样本经过投影后：

* 异类样本的中心尽可能远
* 同类样本的方差尽可能小

#### 损失函数推导

经过投影后，**异类样本的中心尽可能远**

$max||w^T\mu_0-w^T\mu_1||^2_2$

注：上式等价于每类的均值投影点尽可能的远。相当于前面乘了常数$|w|$，不影响结果。（$max|||w|*|\mu_0|*cos\theta_0-|w|*|\mu_1|*cos\theta_1||^2_2$）

经过投影后，**同类样本的协方差尽可能小**

$min w^T\Sigma_0w$

$w^T\Sigma_0w=w^T(\sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T)w=\sum_{x\in X_0}(w^Tx-w^T\mu_0)(x^Tw-\mu^T_0w)$

$max J = \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}=\frac{w^TS_bw}{w^TS_ww}$

$S_b$定义为类间散度矩阵，$S_w$为类内散度矩阵

上式等价于 $min -w^TS_bw$， $s.t. w^TS_ww=1$

*解上述等式约束，可利用拉格朗日乘子法进行求解*

* ![image-20220328200903152](https://gitee.com/xjg0216/blogimg/raw/master/img/image-20220328200903152.png)

#### 求解w

由朗格朗日乘子法可得拉格朗日函数为：

$L(w,\lambda)=-w^TS_bw+\lambda(w^TS_ww-1)$

对w求偏导可得，

$\frac{\partial L(w,\lambda)}{\partial w}=-\frac{\partial(w^TS_bw)}{\partial w} + \lambda\frac{\partial(w^TS_ww-1)}{\partial w}=-(S_b+S_b^T)w + \lambda(S_w+S_w^T)w$

由于$S_b，S_w$是对称矩阵，所以

$\frac{\partial{L(w,\lambda)}}{\partial w} = -2S_bw + 2\lambda S_ww$

令上式等于0，即可得

$S_bw=\lambda S_ww$         $(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw=\lambda S_ww$

令$(\mu_0-\mu_1)^Tw = \gamma$（前面是列向量，后面是行向量，所以点乘为实数）

$\gamma(\mu_0-\mu_1)=\lambda S_ww$

$w=\frac{\gamma}{\lambda}S^{-1}_w(\mu_0-\mu_1)= S^{-1}_w(\mu_0-\mu_1)$

### 5、支持向量机（SVM）

从几何角度，对于线性可分数据集，支持向量机就是**找距离正负样本都最远的超平面**，相比于感知机（只是寻找可以分离正负样本的超平面），其解释唯一的，且不偏不倚，泛化性能更好。

#### 超平面

n维空间的超平面($w^Tx+b=0$,其中$w,x\in R^n$):

* 超平面方程不唯一

* 法向量w和位移项b确定一个唯一超平面

* 法向量w垂直于超平面（缩放w,b时，若缩放倍数为负数会改变法向量方向）

* 法向量w指向的那一半空间为正空间，另一半为负空间

* 任意点x到超平面的距离公式为$r=\frac{|w^Tx+b|}{||w||}$

#### 几何间隔

对于给定的数据集X和超平面$w^Tx+b=0$，定义数据集X中的任意一个样本点

$(x_i,y_i),y_i\in \{-1,1\},i=1,2,...,m$关于超平面的几何间隔为：

$\gamma_i=\frac{y_i(w^Tx_i+b)}{||w||}$

正确分类时，$\gamma_i>0$，几何间隔此时也等价于点到超平面的距离

没有正确分类时，$\gamma_i<0$

对于给定的数据集X和超平面$w^Tx+b=0$,定义数据集X关于超平面的几何间隔为：

**数据集X中所有样本点的几何间隔最小值**

$\gamma = min_{i=1,2...,m} \gamma_i$

#### 支持向量机模型

模型：给定线性可分数据集X，支持向量机模型希望求得数据集X关于超平面的几何间隔$\gamma$

达到最大的那个超平面，然后套上一个sign函数实现分类功能

$$
y=sign(w^Tx+b)=\begin{cases}

1 & w^Tx + b > 0\\

-1 & w^Tx+b < 0

\end{cases}
$$

所以其本质和感知机一样，仍然是在求一个超平面，**几何间隔最大的超平面一定就是我们前面所说的距离正负样本都最远的超平面**

原因有两点：

* 当超平面没有正确划分正负样本时：几何间隔最小的为误分类点，因此$\gamma<0$

* 当超平面正确划分超平面时：$\gamma >= 0$,且越靠近中央$\gamma$越大

##### 策略与求解

给定线性可分数据集X，设X中几何间隔最小的样本为$(x_{min},y_{min})$，那么支持向量机找超平面的过程可以转化为以下带约束条件的优化问题

![](https://gitee.com/xjg0216/blogimg/raw/master/img/2022-03-29-16-01-09-image.png)

假设该问题的最优解为$(w^*,b^*)$，那么$(\alpha w^*,\alpha b^*)$也是最优解，且超平面也不变，因此还需要对$w,b$做一定的限制才能使得上述优化问题有可解的唯一解，不仿设分子$y_{min}(w^Tx_{min}+b)=1$，因为对于特定的$(x_{min},y_{min})$来说，能使得$y_{min}(w^Tx_{min}+b)=1$的$\alpha$有且只有一个，因此上述问题进一步转化为：

![](https://gitee.com/xjg0216/blogimg/raw/master/img/2022-03-29-16-10-27-image.png)

由于求$1/||w||$的最大值相当于求$\frac{1}{2}||w||^2$的最小值，所以上述目标函数等价于:

![](https://gitee.com/xjg0216/blogimg/raw/master/img/2022-03-29-16-13-20-image.png)

因为现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题，可以通过拉格朗日对偶性变换到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：对偶问题往往更容易求解，二是可以自然的引入核函数，进而推广到非线性分类问题。

参考以下：

* ![](https://gitee.com/xjg0216/blogimg/raw/master/img/2022-03-29-17-12-54-image.png)

* ![](https://gitee.com/xjg0216/blogimg/raw/master/img/2022-03-29-17-15-08-image.png)

* ![](https://gitee.com/xjg0216/blogimg/raw/master/img/2022-03-29-17-16-01-image.png)

拉格朗日对偶函数$\Gamma(\mu,\lambda)$为拉格朗日函数$L(x,\mu,\lambda)$的下确界。同时“弱对偶性”成立，进而参考主问题满足充分条件，得出该问题满足强对偶性成立，故满足KKT条件。

![](https://gitee.com/xjg0216/blogimg/raw/master/img/1.jpg)

### 6、聚类（无监督）

聚类：物以类聚，将相似的样本聚集到一起，使得同一类簇的样本尽可能接近，不同类簇的样本尽可能远离。

对“距离”的定义：
1、非负性：$dist(x_i,x_j) >=0$

2、同一性：$dist(x_i,x_j)=0$当且仅当$x_i=x_j$

3、对称性： $dist(x_i,x_j)=dist(x_j,x_i)$

4、直递性：$dist(x_i,x_j)<=dist(x_i,x_k)+dist(x_k, x_j)$

**常用的距离度量-连续/离散有序**

明可夫斯基距离：

$dist_{mk}(x_i, x_j)=(\displaystyle\sum^n_{u=1}|x_{iu}-x_{ju}|^p)^{\frac{1}{p}}$

p=2退化为欧式距离：

$dist_{ed}(x_i,x_j)=||x_i-x_j||_2=(\displaystyle\sum^n_{u=1}|x_{iu}-x_{ju}|^2)^{\frac{1}{2}}$

p=1退化为曼哈顿距离：

$dist_{man}(x_i,x_j)=||x_i-x_j||_1=\displaystyle\sum^n_{u=1}|x_{iu}-x_{ju}|$

****

**常用的距离度量-离散无序**

VDM(Value Difference Metric)度量

#### **Kmeans(原型聚类）**

原型指的是类结构能通过一组典型的特例刻画，比如男、女类似的。

给定样本集$D=\{x_1, x_2, ..., x_m\}$，k均值算法针对聚类所得簇划分$C={c_1,c_2,c_k}$,求解最小化平方误差问题：

$E=\displaystyle\sum^k_{i=1}\sum_{x\in C_i}||x-\mu||^2_2$

其中$\mu_i=\frac{1}{|C_i|}\sum_{x \in C_i}$，x表示的是簇$C_i$的均值向量

求解该式需要考虑样本集D所有可能的划分，是一个NP-hard问题，一般来说，我们采用迭代算法求解近似划分。

![image-20220410110332802](https://gitee.com/xjg0216/blogimg/raw/master/img/image-20220410110332802.png)

### 7、降维

#### k近邻学习（kNN）

kNN：近朱者赤，基于与待测样本最近的k个样本的信息进行预测

1NN：最近邻分类器。待测样本标签和与之最近的样本标签一致

给定测试样本x，若与之最近邻样本为z， 则最近邻分类器出错的概率为：

$P(err)=1-\displaystyle\sum_{c\in y}P(c|x)P(c|z)$

![image-20220410112650011](https://gitee.com/xjg0216/blogimg/raw/master/img/image-20220410112650011.png)

