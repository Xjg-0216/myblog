### Semi-Supervised Image Deraining using  Guassian  Process

### Syn2real Transfer Learning for Image Deraining using Gaussian Process

### 利用高斯过程进行半监督图像去雨

#### 摘要

最近基于CNN的图像去雨方法在重建误差和视觉质量方面都取得了良好的性能， 然而这些方法的局限性在于，它们只能在全监督的数据上进行训练，由于获取真实世界的全标记图像去雨数据集的各种挑战，现有的方法只对合成的数据集进行训练，因此很难推广到现实的图像。在训练图像去雨网络中使用真实的雨天图像的文献较少，我们提出了一种**基于高斯过程的半监督学习框架 **，该框架使网络在使用合成数据集进行学习，同时更好地使用未标记的真实图像进行推广。更具体地说，我们使用高斯过程对未标记数据的潜在空间向量进行建模，然后使用该过程计算伪标签，以监督未标记数据上的网络。通过对几个具有挑战性的数据集（Rain800、Rain200L和DDN-SIRR）的广泛实验和消融，我们表明所提出的方法能够有效地利用未标记的数据，从而获得更好的性能。此外，我们还证明了在所提出的基于GP的框架中使用未标记的真实图像比现有的方法具有更优越的性能。

#### 前言

在雨天条件下拍摄的图像通常质量往往很差，雨条纹引入的伪影对后续计算机视觉算法的性能的产生不利影响。

去雨任务受到两个问题的干扰：

* 雨条纹的规模、密度和方向的变化很大
* 缺乏真实雨天图像的标记训练数据

由于目前的大部分网络为完全监督的网络，而且它们只能利用完全的标记训练数据，它们的本质上是有限的。然而，获取有标记的真实的雨天图像是相当具有挑战性的。因此，现有的方法通常只在生成的雨数据集上训练它们的网络。

合成数据集的使用导致了真实图像的性能不够，通常是因为合成图像和多雨图像之间的分布偏移，尽管在性能上存在这种差距，但这个问题在文献中仍没有被探索。

最近，Wei等人提出了一种半监督学习框架（SIRR），他们同时从标记和未标记的数据中学习，以实现图像去雨。对于标记数据的训练，他们使用全监督的方式（预测的与gt之间的平均绝对误差损失）。对于未标记的数据，他们通过对高斯混合模型（GMM）施加的似然项来模拟残差（输入和输出之间的差异）。此外，他们通过最小化它们之间的KL散度差异，加强了额外的一致性，即合成雨的分布更接近真实雨的分布。这是第一个在半监督学习框架中制定图像去雨任务的方法，该框架可以利用未标记的真实数据来提高泛化能力。虽然该方法得到了很好的效果，但它存在以下缺点：

1）由于雨残差的多模态性质，作者假设他们可以用GMM进行建模，只有当实际的残差被用于计算GMM参数时， 这一点才成立。然而，作者在训练迭代中使用真实雨天图像（未标记）图像预测的雨残差进行训练迭代来建模GMM。然后，使用相同的模型计算在随后的迭代中预测的残差（未标记）的可能性。因此，**如果在初始迭代过程中学习到的GMM参数不准确，这很可能是在训练的早期阶段出现的情况，这将导致性能的下降**

2）使用KL散度的目的是使合成的雨分布更接近实际的雨分布，在训练的早期阶段，对真实降雨残差的预测并不准确，因此，尽量减少两种分布之间的差异可能是不合适的

3）使用GMM对降雨残差进行建模，需要人们选择混合组分的数量，从而使模型对这些选择非常敏感。

通过克服这些方法的缺点，我们解决了将未标记的真实图像合并到训练过程中以更好地泛化问题。与Wei相比，我们使用非参数方法来生成对未标记数据的监督，具体地说，我们提出了一种基于高斯过程（GP）的半监督方法学习框架（SSL），这包括对**有标签**和**没有标签**的数据进行迭代训练。

**有标签阶段**：使用预测的和gt之间的均方误差对标记数据进行训练。此外，输入（有标签的数据集）被投影到潜在空间上，然后使用GP进行建模。

**无标签阶段**：我们使用早期在有标签阶段建模的GP为未标记输入生成gt（**伪标签**），然后使用这个伪标签来监督未标记的数据的中间潜在空间。

伪标签的创建是基于这样的假设，即**当未标记图像投影到潜在空间时， 可以表示为标记数据特征的加权组合**，其中权重使用核函数确定，这些权重表明了用于表示未标记数据点的标记数据点的不确定性。因此，**最小化未标记数据投影和伪标签之间的误差可以减少方差，从而导致网络权值自动适应于未标记数据的域**。

主要贡献：

* 我们提出了一种非参数的方法来执行半监督框架，将未标记的真实数据合并到训练过程中
* 该方法利用GP对网络中的中间潜在空间进行建模，然后利用GP为未标记数据创建伪标签，伪标签进一步用于对未标记数据的中间级网络进行监督
* 通过在不同数据集上的大量实验，我们表明，与使用完整训练数据训练的网络相比，该方法在有限的训练数据下能够达到相同的性能，此外，我们还表明，与现有的方法相比，使用所提出的基于GP的半监督框架将未标记的真实数据合并到训练过程中，结果具有更好的性能。

#### 半监督学习

在半监督学习中，我们得到一个输入-目标对的标记数据集($\{x, y\} \in D_L$)，该输入-目标对从一个未知的联合分布$p(x,y)$中取样得到，未标记的输入数据$x \in D_u$从$p(x)$中采样。

目标是学习一个由$\theta$参数化的函数$f(x|\theta)$，它可以准确地预测从$p(x)$中未知的样本的正确目标$y$，参数$\theta $是通过同时利用已标记的和未标记的数据集来学习的。

由于标记的数据集由输入-目标对组成，通常使用平均绝对误差或者交叉熵来训练网络，利用$D_u$的未标记数据，通过不同的技术，如执行一致化， 虚拟对抗训练或者伪标签，增强$f(x|\theta)$关于数据流形的类似$p(x)$形状的结构信息。

我们使用半监督学习框架来利用未标记的真实雨天数据来获得更好的泛化性能。具体地说，我们将由输入-目标对合成的雨数据集作为标记数据集$D_L$，将真实的雨天数据集作为未标记的数据集$D_U$，采用伪标签的方法来利用未标记的数据。

# 高斯过程

GP是一些关于连续域的随机变量的集合，其中任意有限个不同的时间或空间上的随机变量的联合都是高斯分布

#### 高斯过程

一个高斯过程(GP) $f(v)$是一个随机变量的无限集合，其中任何有限的子集都是联合高斯分布的，一个GP完全由它的均值函数和协方差函数指定， 其定义如下：

$m(v) = E[f(v)]$

$K(v, v^{'}) = E[(f(v) - m(v))(f(v^{'}) - m(v^{'}))]$

其中，$v, v^{'} \in V$表示索引GP的可能输入，协方差矩阵是由一个协方差函数或核， K构造的，它表达了底层函数的平滑性的一些先验概念，GP可以表示为如下：（**捕捉不同输入点之间的关系，并且反映在之后样本的位置上**）

$f(v) \sim GP(m(v);K(v, v^{'})+\sigma^2_\epsilon I)$

其中，I为单位矩阵，$\sigma^2_\epsilon$加性噪声方差，任何函数值的集合都是联合高斯分布：

$f(V)=[f(v_1, ..., f(v_n))]^T \sim N(\mu, K(V, V^{'})+\sigma^2_\epsilon I)$

采用均值向量和协方差矩阵定义GP， 为了在未标记的点上进行预测，我们可以通过调整观测数据来计算一个封闭形式的高斯后验分布。

#### 提出的方法

该方法是由一个基于UNet结构的CNN组成，其中每一块都使用Res2Block构建。网络架构的细节在补充材料中提供，总之，该网络由一个编码器$h(x, \theta_{enc})$和一个解码器$g(z, \theta_{dec})$，在这里，编码器和解码器分别由$\theta_{enc}$和$\theta_{dec}$参数化，x是网络的输入，然后由编码器映射到一个潜在向量z。在我们的例子中，x是我们想要去除雨线的雨天图像，我们将潜在向量输入到解码器中产生输出r:r就是雨条纹，从而从雨图像(x)中减去雨条纹分量，生成干净的图像(y)

在我们的问题公式中，训练数据集是$D = D_L \or D_U$, 其中$D_L=\{x^i_l, y^i_l\}^{N_l}_{i=1}$: 由$N_l$抽样， **有标签的数据集**，

$D_U=\{X^i_u\}^{N_u}_{i=1}$:从$N_u$中抽样， **无标签数据集**

该方法的目的是通过同时利用有标记的数据集和没有标记的数据集来学习网络参数，训练过程迭代了两种数据集，网络参数是通过最小化有标记训练阶段的监督损失函数和无标记的非监督损失函数。对于无标记阶段训练，我们使用GP公式生成伪GT， 然后再无监督损失函数中使用。

##### 有标签训练阶段

在这个阶段，我们使用标记的数据$D_L$来学习网络参数，具体来说，我们最小化以下监督损失函数

$L_{sup} = L_1+\lambda_p L_p$

$\lambda_p$是常数， $L_1$和$L_p$是$L_1$损失和监督损失。

除了最小化上述的损失函数，我们还存储了有标签训练数据$x^{i}_{l}$ 的所有中间特征向量（潜在空间向量）$z^i_l$， 它保存在一个矩阵中$F_{zl}$，显然，$F_{zl} = \{z^i_l\}^{N_l}_{i=1}$，它将在稍后的未标记训练阶段，用于生成未标记数据的伪标签，在我们的例子中，$z^i_l$是一个尺度为1xM的向量， 其中，M=32768， 因此，$F_{zl}$是一个尺度为$N_l \star M$

##### 无标签训练阶段

在此阶段，我们利用未标记的数据$D_U$来提高泛化性能，具体地说，我们通过最小化**预测的潜在向量**和**伪标签**（利用GP对标记数据集的潜在空间向量$F_{zl}$和$z^{pred}_{u}$进行建模，得到的伪标签）

###### 利用GP获取伪标签

训练以迭代的方式进行，我么首先使用标记的数据$D_L$来学习权重，然后使用未标记的数据$D_U$对权重进行更新。在对$D_L$进行第一次迭代后，我们将标记数据的潜在空间向量存储在一个列表里$F_{zl}$。这些向量位于一个低的维度上。在未标记阶段，我们将未标记输入的潜在空间向量$(Z_u)$投影到已标注的向量$F_{zl} = \{z^i_l\}^{N_l}_{i=1}$的空间上，即我们将$D_U$的第k个训练样本对应的未标记潜在空间向量$z^k_u$表示为：

$$z^k_u = \sum^{N_l}_{i=1}\alpha_i z^i_l+\epsilon$$

利用这个公式，我们可以用GP联合模拟标记样本和未标记样本的潜在空间向量的分布，调整联合分布将产生以下条件多元高斯分布。

$P(z^k_u|D_L,F_{zl})=N(\mu^k_u, \sum^k_u)$

其中，$\sigma^2_\epsilon$设置为1， K为核函数， $F_{zl}$包含了所有标记图像的潜在空间向量，$K(F_{zl}, F_{zl})$的矩阵大小为$N_l \star N_l$, $K(z^k_u, F_{zl})$是一个尺度为1x$N_l$的向量。

使用所有的向量可能不一定是最优的。原因如下：

i) 这些向量将对应于图像中的不同区域，在雨条纹的内容和密度或方向又广泛的多样性，重要的是只考虑那些与未标记向量相似的向量。

ii) 使用所有的向量在计算上都是不允许的，因此，我们只使用对应于未标记向量的$N_n$个最近的标记向量，更具体地说，我们用$F_{zl}$代替$F_{zl}$。在这里，$F_{zl,n}=\{z^j_l:z^j_l \in nearest(z^k_u,F_{zl},N_n)\}$

我们使用公式（13）预测的均值作为伪标签($z^k_{u,pseudo}$)，在潜在空间水平上进行监督。通过最小化$z^k_{u,pred}=h(x_u,\theta_{enc})$和($z^k_{u,pseudo}$)的损失，我们更新了编码器的权值，从而使网络适应于未标记的数据，从而得到更好的泛化。我们还通过公式（14）最小化预测的方差

利用GP， 我们$F_{zl}$潜在空间向量逼近未标记图像的潜在向量$z^k_u$，通过这样做，我们可能会因为潜在向量之间不相似的情况而预测出不正确的伪标签，这种不同是由于雨条纹的不同组成，如不同的密度，形状和方向，为了解决这个问题，我们使用GP最小化了方差$\sum^k_{u,n}$，此外，我们最大化了$\sum^k_{u,f}$，为了确保$F_{zl}$中的潜在空间向量与未标记的向量$z^k_u$不同，且不影响GP预测，定义如下：
